{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeenuFrancis/flowers_multiclass_CNN/blob/main/Flower_multiclass_CNN_with_Github.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DR-eO17geWu"
      },
      "source": [
        "# Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6U6sqKu7Hfe",
        "outputId": "f17a74a7-a6af-4a3e-b81f-9c40fb76cda4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'flowers_multiclass_CNN'...\n",
            "remote: Enumerating objects: 1148, done.\u001b[K\n",
            "remote: Counting objects: 100% (477/477), done.\u001b[K\n",
            "remote: Compressing objects: 100% (459/459), done.\u001b[K\n",
            "remote: Total 1148 (delta 32), reused 429 (delta 16), pack-reused 671\u001b[K\n",
            "Receiving objects: 100% (1148/1148), 55.04 MiB | 27.33 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone \"https://github.com/NeenuFrancis/flowers_multiclass_CNN.git\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMefrVPCg-60"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sCV30xyVhFbE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FIleuCAjoFD8",
        "outputId": "8fb5fb3f-20c2-48ba-9ad9-6f5a5f4051f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.14.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go-LCQvzt56X"
      },
      "source": [
        "# **Part 1 - Data Preprocessing**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nN0vlOTvHLn"
      },
      "source": [
        "### Preprocessing the Training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YVHOAkIBQwT2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5K8vBUFSt-sG",
        "outputId": "7e1ce82f-20f6-4e5c-ea2e-8a6c037057bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 500 images belonging to 5 classes.\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing import image\n",
        "#Creating an object of ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True,\n",
        "                                   validation_split=0.1)\n",
        "\n",
        "#Generating batches of Augmented data\n",
        "training_set = train_datagen.flow_from_directory('/content/flowers_multiclass_CNN/data/train',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'categorical'\n",
        "                                                 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ikhQLjsvT8u"
      },
      "source": [
        "### Preprocessing the Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c_CryLkvS1H",
        "outputId": "075021b3-8717-4960-e4c5-6d3fa9dded8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 419 images belonging to 5 classes.\n"
          ]
        }
      ],
      "source": [
        "test_datagen = ImageDataGenerator(rescale = 1./255,validation_split=0.1)\n",
        "\n",
        "test_set = test_datagen.flow_from_directory('/content/flowers_multiclass_CNN/data/test',\n",
        "                                             target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'categorical'\n",
        "                                            )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K56vilF6xdcn"
      },
      "source": [
        "# **Part 2 - Building the CNN**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxc2uA8Gx1Oh"
      },
      "source": [
        "### Initialising the CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7wuj966mx2sB"
      },
      "outputs": [],
      "source": [
        "cnn = tf.keras.models.Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqoFz1J6yDB1"
      },
      "source": [
        "### Step 1 - Convolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YW2xKjlSyD29"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu',\n",
        "                               input_shape=[64, 64, 3]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQqI04ekyI99"
      },
      "source": [
        "### Step 2 - Pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jt4KfyrvyKOW"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1AJMfiayQkW"
      },
      "source": [
        "### Adding a second convolutional layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NWwL4UtvySl-"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8LUay2eyX2V"
      },
      "source": [
        "### Step 3 - Flattening"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Rt6DoMidyYtM"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Flatten())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV4ofYcYyezd"
      },
      "source": [
        "### Step 4 - Full Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bSTx2oViybKF"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cRM8YV_ylsl"
      },
      "source": [
        "### Step 5 - Output Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FFLwj12FymeN"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=5, activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHBcsTSSyrcl"
      },
      "source": [
        "# **Part 3 - Training the CNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BbMojc4ywzd"
      },
      "source": [
        "### Compiling the CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NBZ4ugEayxuf"
      },
      "outputs": [],
      "source": [
        "cnn.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utSxqtJay38m"
      },
      "source": [
        "### Training the CNN on the Training set and evaluating it on the Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKvF7gO33zqO",
        "outputId": "37e515d6-c6cb-4de0-e9a4-2f0fb94bf613"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "16/16 [==============================] - 15s 150ms/step - loss: 1.4726 - accuracy: 0.3860 - val_loss: 1.2221 - val_accuracy: 0.4726\n",
            "Epoch 2/25\n",
            "16/16 [==============================] - 2s 133ms/step - loss: 1.1907 - accuracy: 0.5220 - val_loss: 0.9750 - val_accuracy: 0.5871\n",
            "Epoch 3/25\n",
            "16/16 [==============================] - 2s 136ms/step - loss: 1.0310 - accuracy: 0.6040 - val_loss: 0.9636 - val_accuracy: 0.5943\n",
            "Epoch 4/25\n",
            "16/16 [==============================] - 2s 134ms/step - loss: 0.9918 - accuracy: 0.6200 - val_loss: 1.0752 - val_accuracy: 0.5680\n",
            "Epoch 5/25\n",
            "16/16 [==============================] - 2s 132ms/step - loss: 0.9178 - accuracy: 0.6300 - val_loss: 0.9917 - val_accuracy: 0.6181\n",
            "Epoch 6/25\n",
            "16/16 [==============================] - 3s 182ms/step - loss: 0.9505 - accuracy: 0.6320 - val_loss: 0.9585 - val_accuracy: 0.6158\n",
            "Epoch 7/25\n",
            "16/16 [==============================] - 2s 132ms/step - loss: 0.8376 - accuracy: 0.6820 - val_loss: 0.8624 - val_accuracy: 0.6754\n",
            "Epoch 8/25\n",
            "16/16 [==============================] - 2s 132ms/step - loss: 0.7959 - accuracy: 0.7020 - val_loss: 0.9181 - val_accuracy: 0.6492\n",
            "Epoch 9/25\n",
            "16/16 [==============================] - 2s 136ms/step - loss: 0.6877 - accuracy: 0.7680 - val_loss: 0.8723 - val_accuracy: 0.6706\n",
            "Epoch 10/25\n",
            "16/16 [==============================] - 3s 180ms/step - loss: 0.6649 - accuracy: 0.7420 - val_loss: 0.8954 - val_accuracy: 0.6635\n",
            "Epoch 11/25\n",
            "16/16 [==============================] - 2s 132ms/step - loss: 0.6422 - accuracy: 0.7620 - val_loss: 0.9861 - val_accuracy: 0.6468\n",
            "Epoch 12/25\n",
            "16/16 [==============================] - 2s 134ms/step - loss: 0.6323 - accuracy: 0.7660 - val_loss: 0.8304 - val_accuracy: 0.6826\n",
            "Epoch 13/25\n",
            "16/16 [==============================] - 2s 135ms/step - loss: 0.5534 - accuracy: 0.8140 - val_loss: 0.9892 - val_accuracy: 0.6683\n",
            "Epoch 14/25\n",
            "16/16 [==============================] - 3s 162ms/step - loss: 0.5161 - accuracy: 0.8120 - val_loss: 0.9665 - val_accuracy: 0.6706\n",
            "Epoch 15/25\n",
            "16/16 [==============================] - 3s 164ms/step - loss: 0.5015 - accuracy: 0.8060 - val_loss: 0.8983 - val_accuracy: 0.6802\n",
            "Epoch 16/25\n",
            "16/16 [==============================] - 2s 142ms/step - loss: 0.5044 - accuracy: 0.8280 - val_loss: 0.9911 - val_accuracy: 0.6468\n",
            "Epoch 17/25\n",
            "16/16 [==============================] - 2s 129ms/step - loss: 0.4986 - accuracy: 0.8120 - val_loss: 0.9638 - val_accuracy: 0.6563\n",
            "Epoch 18/25\n",
            "16/16 [==============================] - 3s 177ms/step - loss: 0.4071 - accuracy: 0.8760 - val_loss: 0.9283 - val_accuracy: 0.7088\n",
            "Epoch 19/25\n",
            "16/16 [==============================] - 2s 131ms/step - loss: 0.3706 - accuracy: 0.8560 - val_loss: 1.2513 - val_accuracy: 0.5967\n",
            "Epoch 20/25\n",
            "16/16 [==============================] - 2s 131ms/step - loss: 0.3676 - accuracy: 0.8680 - val_loss: 1.0246 - val_accuracy: 0.6563\n",
            "Epoch 21/25\n",
            "16/16 [==============================] - 2s 131ms/step - loss: 0.3432 - accuracy: 0.8880 - val_loss: 1.0486 - val_accuracy: 0.6563\n",
            "Epoch 22/25\n",
            "16/16 [==============================] - 2s 129ms/step - loss: 0.2859 - accuracy: 0.9080 - val_loss: 1.0609 - val_accuracy: 0.6754\n",
            "Epoch 23/25\n",
            "16/16 [==============================] - 3s 201ms/step - loss: 0.2809 - accuracy: 0.8880 - val_loss: 0.9431 - val_accuracy: 0.7088\n",
            "Epoch 24/25\n",
            "16/16 [==============================] - 2s 136ms/step - loss: 0.2215 - accuracy: 0.9320 - val_loss: 1.0550 - val_accuracy: 0.6683\n",
            "Epoch 25/25\n",
            "16/16 [==============================] - 2s 135ms/step - loss: 0.2022 - accuracy: 0.9280 - val_loss: 1.1111 - val_accuracy: 0.6826\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f67f904fc10>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMwwIWduEx18",
        "outputId": "93fa2860-5919-4bb0-a718-1ff0be41f179"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 156ms/step\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "\n",
        "test_image = tf.keras.utils.load_img('/content/flowers_multiclass_CNN/Predict_data/dandelion.jpg',\n",
        "                                     target_size = (64, 64))\n",
        "#test_image = tf.keras.utils.load_img(test_image)\n",
        "im=test_image\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "\n",
        "result = cnn.predict(test_image)\n",
        "\n",
        "training_set.class_indices\n",
        "\n",
        "\n",
        "label=[\"daisy\", \"dandelion\", \"rose\", \"sunflower\", \"tulip\"]\n",
        "for i in label:\n",
        "    if result[0][label.index(i)]==1:\n",
        "         break\n",
        "\n",
        "\n",
        "\n",
        "prediction=i\n",
        "\n",
        "\n",
        "im.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "y35gmHWmSfi_",
        "outputId": "1dc91f74-584c-41d7-ae80-ffc94adf6a39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dandelion\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=64x64>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "print(prediction)\n",
        "\n",
        "display(im)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}